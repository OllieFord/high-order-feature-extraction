{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import syllapy\n",
    "from re import search\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/Gutenberg/txt/Oscar Wilde___The Picture of Dorian Gray.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"hello i like the the the the large boulder jumps.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n",
      "i\n",
      "the large boulder\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(test)\n",
    "\n",
    "cleaned_book = []\n",
    "number_sentences = len(list(doc.sents))\n",
    "for sent in doc.sents:\n",
    "    out = [word.lemma_ for word in sent if word.pos_ in ('VERB', 'NOUN', \"VERB\", 'ADJ', 'ADV')]\n",
    "    \n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'large', 'boulder', 'jump']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[like, large, boulder, jump]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %time a, b = new(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6387"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 s, sys: 2.48 s, total: 32 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%time c, d = stop_content_lemma_sents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(raw_book):\n",
    "    stopwords_dict = Counter(STOP_WORDS)\n",
    "    return ' '.join([word for word in raw_book.split() if word not in stopwords_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_senteces(raw_book):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(raw_book)\n",
    "    number_sentences = len(list(doc.sents))\n",
    "    return doc, number_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization(doc):\n",
    "    cleaned_book = []\n",
    "    for sent in doc.sents:\n",
    "        out = [word.lemma_ for word in sent if word.pos_ in ('VERB', 'NOUN', \"VERB\", 'ADJ', 'ADV')]     \n",
    "        if len(out) >= 1:\n",
    "            cleaned_book.append(out)\n",
    "        else:\n",
    "            pass\n",
    "    return cleaned_book, number_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_content_lemma_sents(raw_book):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(raw_book)\n",
    "\n",
    "    cleaned_book = []\n",
    "    number_sentences = len(list(doc.sents))\n",
    "    for sent in doc.sents:\n",
    "        cleaned_sentence = []\n",
    "        \n",
    "        sent = [word.lemma_ for word in sent if word.pos_ in ('VERB', 'NOUN', \"VERB\", 'ADJ', 'ADV')]\n",
    "        for token in sent:\n",
    "            if token.is_stop:\n",
    "                pass\n",
    "            else:\n",
    "                if token.pos_ == 'VERB' or token.pos_ == 'NOUN' or token.pos_ == 'VERB' or token.pos_ == 'ADJ' or token.pos_ == 'ADV':\n",
    "                    cleaned_sentence.append(token.lemma_)\n",
    "                    \n",
    "        if len(cleaned_sentence) >= 1:\n",
    "            cleaned_book.append(cleaned_sentence)\n",
    "        else:\n",
    "            pass\n",
    "    return cleaned_book, number_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data, num_sentences = stop_content_lemma_sents(\"the quick brown fox fox fox fox fox jumps over the lazy dog. lots of small chickens were playing in the field.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(book):\n",
    "    return np.average([len(word) for word in book])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_word_len = np.average([average_word_length(sent) for sent in cleaned_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_ttr(book):\n",
    "    '''\n",
    "    log type token ration accounts for differences in sequence lengths\n",
    "    a value of 1 means all words are unique \n",
    "    a value of 0 means there are no unique words \n",
    "    \n",
    "    expects: an list of sentences\n",
    "    returns: log(ttr) --> log(token)/log(type)\n",
    "    '''\n",
    "    words = {}\n",
    "    total = 0\n",
    "    for sent in book:\n",
    "        for word in sent:\n",
    "            if word in words:\n",
    "                words[word] += 1\n",
    "                total +=1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "                total +=1\n",
    "    return np.log(len(words))/np.log(total)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllables(cleaned_data):\n",
    "    '''\n",
    "    Counts the sylablys per word \n",
    "    expects: a list of sentences\n",
    "    \n",
    "    returns: a list of sentences where each word has been replaced by its sylabble count \n",
    "    '''\n",
    "    book_syll = []\n",
    "    for sent in cleaned_data:\n",
    "        sent_syll = []\n",
    "        for word in sent:\n",
    "            sent_syll.append(syllapy.count(word))\n",
    "        book_syll.append(sent_syll)\n",
    "    return book_syll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 2, 1], [1, 1, 2, 1, 1]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllables(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8854692840710255"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_ttr(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenome_scores = {\"a\": 10, \"e\": 9, \"o\": 9,\n",
    "                  \"i\": 8, \"u\": 8,\"j\": 8,\"w\": 8,\n",
    "                  \"r\": 7,\n",
    "                  \"l\": 6,\n",
    "                  \"m\": 5,\"n\": 5,\"ng\": 5,\n",
    "                  \"z\": 4,\"v\": 4,\n",
    "                  \"f\": 3,\"th\": 3,\"s\": 3,\n",
    "                  \"b\": 2,\"d\": 2,\"g\": 2,\n",
    "                    \"p\": 1,\"t\": 1,\"q\": 1,\"x\": 1,\"z\": 1,\"y\": 1,\"c\":1,\"k\":1,\"h\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sonority\n",
    "def phenomes(cleaned_data):\n",
    "    '''\n",
    "    Assigns an score to each word in a sentence based on english phenome heirarchy.\n",
    "    Expects: list of sentences \n",
    "    returns: list of sentences where each word has been replaced by its phenome score \n",
    "    '''\n",
    "    book_score = []\n",
    "    for sent in cleaned_data:\n",
    "        sent_score = []\n",
    "        for word in sent:\n",
    "            word_score = 0\n",
    "            skip = []\n",
    "            \n",
    "            if search('th', word):\n",
    "                word_score += phenome_scores['th']\n",
    "                skip.append(search('th', word).span()[0])\n",
    "                skip.append(search('th', word).span()[1]-1)\n",
    "                \n",
    "            if search('ng', word):\n",
    "                word_score += phenome_scores['ng']\n",
    "                skip.append(search('ng', word).span()[0])\n",
    "                skip.append(search('ng', word).span()[1]-1)\n",
    "            \n",
    "            for i,l in enumerate(word):\n",
    "                if i in skip:\n",
    "                    pass\n",
    "                if l in phenome_scores:\n",
    "                    word_score += phenome_scores[l]\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "            sent_score.append(word_score)\n",
    "            \n",
    "        book_score.append(sent_score)\n",
    "    return book_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 31, 13, 13, 13, 13, 13, 22, 18, 13], [16, 30, 26, 18, 28]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phenomes(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_book, sent_counts = stop_content_lemma_sents(data)\n",
    "average_word_len = np.average([average_word_length(sent) for sent in a])\n",
    "average_syllables = np.average([np.average(sent) for sent in syllables(a)])\n",
    "average_phenomes = np.average([np.average(sent) for sent in phenomes(a)])\n",
    "ttr = log_ttr(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6387,\n",
       " 5.505289932296925,\n",
       " 1.6776957154641305,\n",
       " 30.81638851033225,\n",
       " 0.8594171074659054)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_counts, average_word_len, average_syllables, average_phenomes, ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_book, sent_counts = stop_content_lemma_sents(data)\n",
    "average_word_len = np.average([average_word_length(sent) for sent in c])\n",
    "average_syllables = np.average([np.average(sent) for sent in syllables(c)])\n",
    "average_phenomes = np.average([np.average(sent) for sent in phenomes(c)])\n",
    "ttr = log_ttr(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6387,\n",
       " 5.945797565856851,\n",
       " 1.7866634875609775,\n",
       " 33.062045340593585,\n",
       " 0.8742969948730204)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_counts, average_word_len, average_syllables, average_phenomes, ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_book, sent_counts = stop_content_lemma_sents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_word_len = np.average([average_word_length(sent) for sent in cleaned_book])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.945797565856851"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
